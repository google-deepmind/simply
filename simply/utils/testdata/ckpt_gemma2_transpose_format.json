{
  "opt_state": {
    "1": {
      "0": {
        "mu": {
          "transformer/embedder": {
            "input_embedding": [[528, 529, 530, 531], [532, 533, 534, 535]],
            "output_bias": [540, 541]
          },
          "transformer/final_norm": {
            "scale": [536, 537, 538, 539]
          },
          "transformer/layer_0/attn": {
            "attn_vec_einsum": {
              "w": [[[0, 8, 16, 24], [1, 9, 17, 25], [2, 10, 18, 26], [3, 11, 19, 27]], [[4, 12, 20, 28], [5, 13, 21, 29], [6, 14, 22, 30], [7, 15, 23, 31]]]
            },
            "qkv_einsum": {
              "w": [[[[36, 37, 38, 39], [44, 45, 46, 47], [52, 53, 54, 55], [60, 61, 62, 63]], [[40, 41, 42, 43], [48, 49, 50, 51], [56, 57, 58, 59], [64, 65, 66, 67]]], [[[68, 69, 70, 71], [76, 77, 78, 79], [84, 85, 86, 87], [92, 93, 94, 95]], [[72, 73, 74, 75], [80, 81, 82, 83], [88, 89, 90, 91], [96, 97, 98, 99]]], [[[100, 101, 102, 103], [108, 109, 110, 111], [116, 117, 118, 119], [124, 125, 126, 127]], [[104, 105, 106, 107], [112, 113, 114, 115], [120, 121, 122, 123], [128, 129, 130, 131]]]]
            },
            "query_per_dim_scale": {
              "scale": [32, 33, 34, 35]
            }
          },
          "transformer/layer_0/mlp/gating_einsum": {
            "b": [[172, 173, 174, 175, 176, 177, 178, 179], [132, 133, 134, 135, 136, 137, 138, 139]],
            "w": [[[180, 188, 196, 204], [181, 189, 197, 205], [182, 190, 198, 206], [183, 191, 199, 207], [184, 192, 200, 208], [185, 193, 201, 209], [186, 194, 202, 210], [187, 195, 203, 211]], [[140, 148, 156, 164], [141, 149, 157, 165], [142, 150, 158, 166], [143, 151, 159, 167], [144, 152, 160, 168], [145, 153, 161, 169], [146, 154, 162, 170], [147, 155, 163, 171]]]
          },
          "transformer/layer_0/mlp/linear": {
            "b": [212, 213, 214, 215],
            "w": [[216, 217, 218, 219], [220, 221, 222, 223], [224, 225, 226, 227], [228, 229, 230, 231], [232, 233, 234, 235], [236, 237, 238, 239], [240, 241, 242, 243], [244, 245, 246, 247]]
          },
          "transformer/layer_0/post_attention_norm": {
            "scale": [248, 249, 250, 251]
          },
          "transformer/layer_0/post_ffw_norm": {
            "scale": [252, 253, 254, 255]
          },
          "transformer/layer_0/pre_attention_norm": {
            "scale": [256, 257, 258, 259]
          },
          "transformer/layer_0/pre_ffw_norm": {
            "scale": [260, 261, 262, 263]
          },
          "transformer/layer_1/attn": {
            "attn_vec_einsum": {
              "w": [[[264, 272, 280, 288], [265, 273, 281, 289], [266, 274, 282, 290], [267, 275, 283, 291]], [[268, 276, 284, 292], [269, 277, 285, 293], [270, 278, 286, 294], [271, 279, 287, 295]]]
            },
            "qkv_einsum": {
              "w": [[[[300, 301, 302, 303], [308, 309, 310, 311], [316, 317, 318, 319], [324, 325, 326, 327]], [[304, 305, 306, 307], [312, 313, 314, 315], [320, 321, 322, 323], [328, 329, 330, 331]]], [[[332, 333, 334, 335], [340, 341, 342, 343], [348, 349, 350, 351], [356, 357, 358, 359]], [[336, 337, 338, 339], [344, 345, 346, 347], [352, 353, 354, 355], [360, 361, 362, 363]]], [[[364, 365, 366, 367], [372, 373, 374, 375], [380, 381, 382, 383], [388, 389, 390, 391]], [[368, 369, 370, 371], [376, 377, 378, 379], [384, 385, 386, 387], [392, 393, 394, 395]]]]
            },
            "query_per_dim_scale": {
              "scale": [296, 297, 298, 299]
            }
          },
          "transformer/layer_1/mlp/gating_einsum": {
            "b": [[436, 437, 438, 439, 440, 441, 442, 443], [396, 397, 398, 399, 400, 401, 402, 403]],
            "w": [[[444, 452, 460, 468], [445, 453, 461, 469], [446, 454, 462, 470], [447, 455, 463, 471], [448, 456, 464, 472], [449, 457, 465, 473], [450, 458, 466, 474], [451, 459, 467, 475]], [[404, 412, 420, 428], [405, 413, 421, 429], [406, 414, 422, 430], [407, 415, 423, 431], [408, 416, 424, 432], [409, 417, 425, 433], [410, 418, 426, 434], [411, 419, 427, 435]]]
          },
          "transformer/layer_1/mlp/linear": {
            "b": [476, 477, 478, 479],
            "w": [[480, 481, 482, 483], [484, 485, 486, 487], [488, 489, 490, 491], [492, 493, 494, 495], [496, 497, 498, 499], [500, 501, 502, 503], [504, 505, 506, 507], [508, 509, 510, 511]]
          },
          "transformer/layer_1/post_attention_norm": {
            "scale": [512, 513, 514, 515]
          },
          "transformer/layer_1/post_ffw_norm": {
            "scale": [516, 517, 518, 519]
          },
          "transformer/layer_1/pre_attention_norm": {
            "scale": [520, 521, 522, 523]
          },
          "transformer/layer_1/pre_ffw_norm": {
            "scale": [524, 525, 526, 527]
          }
        },
        "nu": {
          "transformer/embedder": {
            "input_embedding": [[1613, 1614, 1615, 1616], [1617, 1618, 1619, 1620]],
            "output_bias": [1625, 1626]
          },
          "transformer/final_norm": {
            "scale": [1621, 1622, 1623, 1624]
          },
          "transformer/layer_0/attn": {
            "attn_vec_einsum": {
              "w": [[[1085, 1093, 1101, 1109], [1086, 1094, 1102, 1110], [1087, 1095, 1103, 1111], [1088, 1096, 1104, 1112]], [[1089, 1097, 1105, 1113], [1090, 1098, 1106, 1114], [1091, 1099, 1107, 1115], [1092, 1100, 1108, 1116]]]
            },
            "qkv_einsum": {
              "w": [[[[1121, 1122, 1123, 1124], [1129, 1130, 1131, 1132], [1137, 1138, 1139, 1140], [1145, 1146, 1147, 1148]], [[1125, 1126, 1127, 1128], [1133, 1134, 1135, 1136], [1141, 1142, 1143, 1144], [1149, 1150, 1151, 1152]]], [[[1153, 1154, 1155, 1156], [1161, 1162, 1163, 1164], [1169, 1170, 1171, 1172], [1177, 1178, 1179, 1180]], [[1157, 1158, 1159, 1160], [1165, 1166, 1167, 1168], [1173, 1174, 1175, 1176], [1181, 1182, 1183, 1184]]], [[[1185, 1186, 1187, 1188], [1193, 1194, 1195, 1196], [1201, 1202, 1203, 1204], [1209, 1210, 1211, 1212]], [[1189, 1190, 1191, 1192], [1197, 1198, 1199, 1200], [1205, 1206, 1207, 1208], [1213, 1214, 1215, 1216]]]]
            },
            "query_per_dim_scale": {
              "scale": [1117, 1118, 1119, 1120]
            }
          },
          "transformer/layer_0/mlp/gating_einsum": {
            "b": [[1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264], [1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224]],
            "w": [[[1265, 1273, 1281, 1289], [1266, 1274, 1282, 1290], [1267, 1275, 1283, 1291], [1268, 1276, 1284, 1292], [1269, 1277, 1285, 1293], [1270, 1278, 1286, 1294], [1271, 1279, 1287, 1295], [1272, 1280, 1288, 1296]], [[1225, 1233, 1241, 1249], [1226, 1234, 1242, 1250], [1227, 1235, 1243, 1251], [1228, 1236, 1244, 1252], [1229, 1237, 1245, 1253], [1230, 1238, 1246, 1254], [1231, 1239, 1247, 1255], [1232, 1240, 1248, 1256]]]
          },
          "transformer/layer_0/mlp/linear": {
            "b": [1297, 1298, 1299, 1300],
            "w": [[1301, 1302, 1303, 1304], [1305, 1306, 1307, 1308], [1309, 1310, 1311, 1312], [1313, 1314, 1315, 1316], [1317, 1318, 1319, 1320], [1321, 1322, 1323, 1324], [1325, 1326, 1327, 1328], [1329, 1330, 1331, 1332]]
          },
          "transformer/layer_0/post_attention_norm": {
            "scale": [1333, 1334, 1335, 1336]
          },
          "transformer/layer_0/post_ffw_norm": {
            "scale": [1337, 1338, 1339, 1340]
          },
          "transformer/layer_0/pre_attention_norm": {
            "scale": [1341, 1342, 1343, 1344]
          },
          "transformer/layer_0/pre_ffw_norm": {
            "scale": [1345, 1346, 1347, 1348]
          },
          "transformer/layer_1/attn": {
            "attn_vec_einsum": {
              "w": [[[1349, 1357, 1365, 1373], [1350, 1358, 1366, 1374], [1351, 1359, 1367, 1375], [1352, 1360, 1368, 1376]], [[1353, 1361, 1369, 1377], [1354, 1362, 1370, 1378], [1355, 1363, 1371, 1379], [1356, 1364, 1372, 1380]]]
            },
            "qkv_einsum": {
              "w": [[[[1385, 1386, 1387, 1388], [1393, 1394, 1395, 1396], [1401, 1402, 1403, 1404], [1409, 1410, 1411, 1412]], [[1389, 1390, 1391, 1392], [1397, 1398, 1399, 1400], [1405, 1406, 1407, 1408], [1413, 1414, 1415, 1416]]], [[[1417, 1418, 1419, 1420], [1425, 1426, 1427, 1428], [1433, 1434, 1435, 1436], [1441, 1442, 1443, 1444]], [[1421, 1422, 1423, 1424], [1429, 1430, 1431, 1432], [1437, 1438, 1439, 1440], [1445, 1446, 1447, 1448]]], [[[1449, 1450, 1451, 1452], [1457, 1458, 1459, 1460], [1465, 1466, 1467, 1468], [1473, 1474, 1475, 1476]], [[1453, 1454, 1455, 1456], [1461, 1462, 1463, 1464], [1469, 1470, 1471, 1472], [1477, 1478, 1479, 1480]]]]
            },
            "query_per_dim_scale": {
              "scale": [1381, 1382, 1383, 1384]
            }
          },
          "transformer/layer_1/mlp/gating_einsum": {
            "b": [[1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528], [1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488]],
            "w": [[[1529, 1537, 1545, 1553], [1530, 1538, 1546, 1554], [1531, 1539, 1547, 1555], [1532, 1540, 1548, 1556], [1533, 1541, 1549, 1557], [1534, 1542, 1550, 1558], [1535, 1543, 1551, 1559], [1536, 1544, 1552, 1560]], [[1489, 1497, 1505, 1513], [1490, 1498, 1506, 1514], [1491, 1499, 1507, 1515], [1492, 1500, 1508, 1516], [1493, 1501, 1509, 1517], [1494, 1502, 1510, 1518], [1495, 1503, 1511, 1519], [1496, 1504, 1512, 1520]]]
          },
          "transformer/layer_1/mlp/linear": {
            "b": [1561, 1562, 1563, 1564],
            "w": [[1565, 1566, 1567, 1568], [1569, 1570, 1571, 1572], [1573, 1574, 1575, 1576], [1577, 1578, 1579, 1580], [1581, 1582, 1583, 1584], [1585, 1586, 1587, 1588], [1589, 1590, 1591, 1592], [1593, 1594, 1595, 1596]]
          },
          "transformer/layer_1/post_attention_norm": {
            "scale": [1597, 1598, 1599, 1600]
          },
          "transformer/layer_1/post_ffw_norm": {
            "scale": [1601, 1602, 1603, 1604]
          },
          "transformer/layer_1/pre_attention_norm": {
            "scale": [1605, 1606, 1607, 1608]
          },
          "transformer/layer_1/pre_ffw_norm": {
            "scale": [1609, 1610, 1611, 1612]
          }
        }
      }
    }
  },
  "params": {
    "transformer/embedder": {
      "input_embedding": [[1070, 1071, 1072, 1073], [1074, 1075, 1076, 1077]],
      "output_bias": [1082, 1083]
    },
    "transformer/final_norm": {
      "scale": [1078, 1079, 1080, 1081]
    },
    "transformer/layer_0/attn": {
      "attn_vec_einsum": {
        "w": [[[542, 550, 558, 566], [543, 551, 559, 567], [544, 552, 560, 568], [545, 553, 561, 569]], [[546, 554, 562, 570], [547, 555, 563, 571], [548, 556, 564, 572], [549, 557, 565, 573]]]
      },
      "qkv_einsum": {
        "w": [[[[578, 579, 580, 581], [586, 587, 588, 589], [594, 595, 596, 597], [602, 603, 604, 605]], [[582, 583, 584, 585], [590, 591, 592, 593], [598, 599, 600, 601], [606, 607, 608, 609]]], [[[610, 611, 612, 613], [618, 619, 620, 621], [626, 627, 628, 629], [634, 635, 636, 637]], [[614, 615, 616, 617], [622, 623, 624, 625], [630, 631, 632, 633], [638, 639, 640, 641]]], [[[642, 643, 644, 645], [650, 651, 652, 653], [658, 659, 660, 661], [666, 667, 668, 669]], [[646, 647, 648, 649], [654, 655, 656, 657], [662, 663, 664, 665], [670, 671, 672, 673]]]]
      },
      "query_per_dim_scale": {
        "scale": [574, 575, 576, 577]
      }
    },
    "transformer/layer_0/mlp/gating_einsum": {
      "b": [[714, 715, 716, 717, 718, 719, 720, 721], [674, 675, 676, 677, 678, 679, 680, 681]],
      "w": [[[722, 730, 738, 746], [723, 731, 739, 747], [724, 732, 740, 748], [725, 733, 741, 749], [726, 734, 742, 750], [727, 735, 743, 751], [728, 736, 744, 752], [729, 737, 745, 753]], [[682, 690, 698, 706], [683, 691, 699, 707], [684, 692, 700, 708], [685, 693, 701, 709], [686, 694, 702, 710], [687, 695, 703, 711], [688, 696, 704, 712], [689, 697, 705, 713]]]
    },
    "transformer/layer_0/mlp/linear": {
      "b": [754, 755, 756, 757],
      "w": [[758, 759, 760, 761], [762, 763, 764, 765], [766, 767, 768, 769], [770, 771, 772, 773], [774, 775, 776, 777], [778, 779, 780, 781], [782, 783, 784, 785], [786, 787, 788, 789]]
    },
    "transformer/layer_0/post_attention_norm": {
      "scale": [790, 791, 792, 793]
    },
    "transformer/layer_0/post_ffw_norm": {
      "scale": [794, 795, 796, 797]
    },
    "transformer/layer_0/pre_attention_norm": {
      "scale": [798, 799, 800, 801]
    },
    "transformer/layer_0/pre_ffw_norm": {
      "scale": [802, 803, 804, 805]
    },
    "transformer/layer_1/attn": {
      "attn_vec_einsum": {
        "w": [[[806, 814, 822, 830], [807, 815, 823, 831], [808, 816, 824, 832], [809, 817, 825, 833]], [[810, 818, 826, 834], [811, 819, 827, 835], [812, 820, 828, 836], [813, 821, 829, 837]]]
      },
      "qkv_einsum": {
        "w": [[[[842, 843, 844, 845], [850, 851, 852, 853], [858, 859, 860, 861], [866, 867, 868, 869]], [[846, 847, 848, 849], [854, 855, 856, 857], [862, 863, 864, 865], [870, 871, 872, 873]]], [[[874, 875, 876, 877], [882, 883, 884, 885], [890, 891, 892, 893], [898, 899, 900, 901]], [[878, 879, 880, 881], [886, 887, 888, 889], [894, 895, 896, 897], [902, 903, 904, 905]]], [[[906, 907, 908, 909], [914, 915, 916, 917], [922, 923, 924, 925], [930, 931, 932, 933]], [[910, 911, 912, 913], [918, 919, 920, 921], [926, 927, 928, 929], [934, 935, 936, 937]]]]
      },
      "query_per_dim_scale": {
        "scale": [838, 839, 840, 841]
      }
    },
    "transformer/layer_1/mlp/gating_einsum": {
      "b": [[978, 979, 980, 981, 982, 983, 984, 985], [938, 939, 940, 941, 942, 943, 944, 945]],
      "w": [[[986, 994, 1002, 1010], [987, 995, 1003, 1011], [988, 996, 1004, 1012], [989, 997, 1005, 1013], [990, 998, 1006, 1014], [991, 999, 1007, 1015], [992, 1000, 1008, 1016], [993, 1001, 1009, 1017]], [[946, 954, 962, 970], [947, 955, 963, 971], [948, 956, 964, 972], [949, 957, 965, 973], [950, 958, 966, 974], [951, 959, 967, 975], [952, 960, 968, 976], [953, 961, 969, 977]]]
    },
    "transformer/layer_1/mlp/linear": {
      "b": [1018, 1019, 1020, 1021],
      "w": [[1022, 1023, 1024, 1025], [1026, 1027, 1028, 1029], [1030, 1031, 1032, 1033], [1034, 1035, 1036, 1037], [1038, 1039, 1040, 1041], [1042, 1043, 1044, 1045], [1046, 1047, 1048, 1049], [1050, 1051, 1052, 1053]]
    },
    "transformer/layer_1/post_attention_norm": {
      "scale": [1054, 1055, 1056, 1057]
    },
    "transformer/layer_1/post_ffw_norm": {
      "scale": [1058, 1059, 1060, 1061]
    },
    "transformer/layer_1/pre_attention_norm": {
      "scale": [1062, 1063, 1064, 1065]
    },
    "transformer/layer_1/pre_ffw_norm": {
      "scale": [1066, 1067, 1068, 1069]
    }
  },
  "step_on_device": 1084
}